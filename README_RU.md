# LLM Proxy Server

Python приложение для проксирования запросов из расширений VS Code, таких как Cline, Roo-Code к различным LLM по API (Anthropic, DeepSeek, Moonshot, OpenRouter, xAI) с OpenAI-compatible интерфейсом. Поддерживает GUI интерфейс, статистику использования, логирование и локализации на 10 языков.

## Архитектура приложения

### Основные компоненты

#### Ядро приложения

- **`main.py`** - Точка входа для запуска GUI интерфейса
- **`server.py`** - Основной FastAPI сервер с логикой проксирования
- **`gui.py`** - Графический интерфейс на tkinter с поддержкой локализаций
- **`config.py`** - Управление настройками через JSON конфигурацию

#### Провайдеры LLM

Расположены в директории `providers/`:

- **`anthropic.py`** - Интеграция с Anthropic Claude
- **`deepseek.py`** - Интеграция с DeepSeek
- **`moonshot.py`** - Интеграция с Moonshot
- **`openrouter.py`** - Интеграция с OpenRouter
- **`xai.py`** - Интеграция с xAI Grok
- **`local.py`** - Локальный провайдер (для тестирования)

#### Утилиты

- **`utils/token_counter.py`** - Подсчет токенов и расчет стоимости запросов

#### Конфигурация

- **`settings.json`** - Автоматически создаваемый файл настроек
- **`requirements.txt`** - Зависимости Python

## Установка

1. Установите зависимости:

```bash
pip install -r requirements.txt
```

2. Запустите приложение:

```bash
python main.py
```

## Настройка провайдеров

Откройте GUI и перейдите в раздел настроек (кнопка ⚙). Для каждого провайдера:

1. Включите провайдер (чекбокс "Enabled")
2. Укажите API ключ
3. Выберите модель из списка доступных

### Поддерживаемые провайдеры

| Провайдер  | Модели                           | Особенности                             |
| ---------- | -------------------------------- | --------------------------------------- |
| Anthropic  | Claude 3.5 Sonnet, Claude 3 Opus | Высокое качество, поддержка reasoning   |
| DeepSeek   | DeepSeek Chat, DeepSeek Coder    | Экономичный, хорошая производительность |
| Moonshot   | Kimi k2                          | Китайский провайдер                     |
| OpenRouter | 100+ моделей                     | Универсальный доступ ко всем моделям    |
| xAI        | Grok                             | Модель от xAI                           |
| Local      | GPT-OSS-120B                     | Локальная модель для тестирования       |

## Использование

### GUI режим (рекомендуется)

1. Запустите `python main.py`
2. Выберите провайдера в выпадающем списке
3. Нажмите "▶ Start" для запуска сервера
4. Сервер будет доступен на `http://localhost:8000`

### Режим сервера

```bash
python server.py
# или
uvicorn server:app --host 0.0.0.0 --port 8000
```

### Альтернативный запуск

```bash
python run_server.py
```

## Локализации

Приложение поддерживает 10 языков интерфейса:

- English (en)
- Русский (ru)
- 中文 (zh)
- Español (es)
- हिन्दी (hi)
- العربية (ar)
- বাংলা (bn)
- Português (pt)
- 日本語 (ja)
- Deutsch (de)

Язык выбирается в настройках GUI.

## Статистика и логирование

### Статистика использования

- Количество обработанных запросов
- Общее количество токенов (вход + выход)
- Расчет стоимости использования для платных провайдеров

### Логирование

- Логи запросов и ответов в GUI
- Сохранение логов в файл (опционально)
- Отображение последних 100 запросов/ответов

## API Endpoints

### Основные endpoints

- `GET /` - Статус сервера
- `GET /health` - Проверка здоровья
- `POST /v1/chat/completions` - Основной endpoint для чата
- `GET /stats` - Статистика сервера
- `GET /logs/requests` - Логи запросов
- `GET /logs/responses` - Логи ответов
- `GET /logs/all` - Все логи

### Управление провайдерами

- `GET /providers` - Список доступных провайдеров
- `POST /switch-provider/{provider_name}` - Переключение провайдера

### Отладка

- `POST /debug/cline` - Отладка запросов от Cline

## Совместимость с IDE расширениями

### VS Code + Cline

1. Установите расширение для VS Code - Cline или Roo-Code
2. В настройках укажите:
   - **Провайдер API** OpenAI Compatible
   - **Base URL:** `http://localhost:8000/v1` (либо IP пк в локальной или внешней сети, например http://192.168.1.44:8000/v1)
   - **API Key:** любой непустой ключ (игнорируется прокси)
   - **Model:** любая модель из списка (игнорируется прокси)

### Другие расширения

Для расширений с поддержкой OpenAI API:

- **Base URL:** `http://localhost:8000` или `http://localhost:8000/v1`
- **API Key:** любой ключ (не используется)

## Особенности

- **Streaming поддержка:** Полная поддержка streaming ответов
- **Мультимодальность:** Поддержка изображений и сложного контента
- **Гибкая настройка:** Легкое добавление новых провайдеров
- **Статистика:** Детальный учет использования и стоимости
- **Локализации:** Поддержка 10 языков интерфейса
- **Совместимость:** OpenAI-compatible API для максимальной совместимости

## Разработка

Для добавления нового провайдера:

1. Создайте файл в `providers/`
2. Реализуйте класс с методом `chat_completion()`
3. Добавьте провайдера в `server.py`
4. Обновите настройки в `config.py` и в `settings.xml`

## Структура файлов

```
proxy_llm/
├── main.py              # Точка входа GUI
├── server.py            # FastAPI сервер
├── gui.py               # Графический интерфейс
├── config.py            # Управление настройками
├── run_server.py       # Альтернативный запуск сервера
├── requirements.txt     # Зависимости
├── settings.json        # Настройки (создается автоматически)
├── providers/           # Модули провайдеров
│   ├── anthropic.py
│   ├── deepseek.py
│   ├── moonshot.py
│   ├── openrouter.py
│   ├── xai.py
│   └── local.py
├── utils/               # Утилиты
│   └── token_counter.py
└── logs/                # Директория для логов (создается автоматически)
```

## Лицензия

MIT License
