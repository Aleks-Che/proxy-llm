
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import time
import logging
import asyncio
from providers.deepseek import DeepSeekProvider
from providers.moonshot import MoonshotProvider
from providers.local import LocalProvider
from utils.token_counter import TokenCounter
from config import Config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# Middleware –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
@app.middleware("http")
async def log_requests(request, call_next):
    client_ip = request.client.host if request.client else "unknown"
    user_agent = request.headers.get("user-agent", "unknown")
    logger.info(f"üîç Request from {client_ip}: {request.method} {request.url}")
    logger.info(f"üîç Headers: {dict(request.headers)}")
    
    # –¢–æ–ª—å–∫–æ –ª–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–ª–µ, –Ω–æ –Ω–µ —á–∏—Ç–∞–µ–º –µ–≥–æ
    if request.method == "POST":
        content_length = request.headers.get("content-length", "unknown")
        logger.info(f"üîç Content-Length: {content_length}")

    try:
        response = await call_next(request)
        logger.info(f"‚úÖ Response: {response.status_code}")
        logger.info(f"‚úÖ Response headers: {dict(response.headers)}")
        return response
    except Exception as e:
        logger.error(f"‚ùå Error in request processing: {e}")
        raise

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ü—Ä–æ–≤–∞–π–¥–µ—Ä—ã
providers = {}
if Config.DEEPSEEK_API_KEY and Config.DEEPSEEK_API_KEY != "sk-–≤–∞—à-–∫–ª—é—á-–∑–¥–µ—Å—å":
    providers["deepseek"] = DeepSeekProvider()
    logger.info("DeepSeek provider initialized")
if Config.MOONSHOT_API_KEY:
    providers["moonshot"] = MoonshotProvider()
    logger.info("Moonshot provider initialized")

# –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
providers["local"] = LocalProvider()
logger.info("Local provider initialized")

current_provider = Config.DEFAULT_PROVIDER if Config.DEFAULT_PROVIDER in providers else "local"
token_counter = TokenCounter()

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: Optional[str] = None
    messages: Optional[List[ChatMessage]] = None
    temperature: Optional[float] = 1.0
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False

@app.get("/")
async def root():
    return {"message": "LLM Proxy Server running", "provider": current_provider}

@app.get("/health")
async def health():
    return {"status": "healthy", "provider": current_provider}

@app.get("/test")
async def test():
    """–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
    try:
        provider = providers[current_provider]
        messages = [{"role": "user", "content": "test"}]
        response = await provider.chat_completion(messages, max_tokens=5)
        return {"status": "ok", "response": response.choices[0].message.content}
    except Exception as e:
        return {"status": "error", "error": str(e)}

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    logger.info("=== START PROCESSING ===")
    logger.info(f"Model requested: {request.model}")
    logger.info(f"Messages count: {len(request.messages) if request.messages else 0}")
    logger.info(f"Stream: {request.stream}")
    logger.info(f"Current provider: {current_provider}")
    
    try:
        provider = providers[current_provider]
        logger.info(f"Using provider: {current_provider}")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
        messages = []
        if request.messages:
            logger.info(f"Raw messages: {request.messages}")
            for i, msg in enumerate(request.messages):
                logger.info(f"Message {i}: role={msg.role}, content_type={type(msg.content)}, content={msg.content[:100]}...")  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
                content = msg.content
                # –ï—Å–ª–∏ content —É–∂–µ —Å—Ç—Ä–æ–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
                if isinstance(content, str):
                    messages.append({"role": msg.role, "content": content})
                # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ (–¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
                elif isinstance(content, list):
                    text_parts = []
                    for item in content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            text_parts.append(item.get("text", ""))
                        elif isinstance(item, str):
                            text_parts.append(item)
                    content = " ".join(text_parts)
                    messages.append({"role": msg.role, "content": content})
                else:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                    messages.append({"role": msg.role, "content": str(content)})
        
        logger.info(f"Processed messages count: {len(messages)}")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        kwargs = {"max_tokens": 1000}
        if request.max_tokens:
            kwargs["max_tokens"] = request.max_tokens
        if request.temperature is not None:
            kwargs["temperature"] = request.temperature
        if request.stream:
            kwargs["stream"] = True
            
        logger.info(f"Calling provider with {len(messages)} messages and kwargs: {kwargs}")
        
        # –í—ã–∑–æ–≤ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        response = await asyncio.wait_for(
            provider.chat_completion(messages, **kwargs),
            timeout=120.0  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        )
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ streaming response
        if request.stream:
            logger.info("Streaming response requested")
            
            # –î–ª—è streaming –æ—Ç–≤–µ—Ç–æ–≤ —Å–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
            from fastapi.responses import StreamingResponse
            import json
            
            async def streaming_generator():
                async for chunk in response:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º chunk –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç
                    if hasattr(chunk, 'to_dict'):
                        chunk_dict = chunk.to_dict()
                    elif hasattr(chunk, 'dict'):
                        chunk_dict = chunk.dict()
                    else:
                        chunk_dict = {
                            "id": f"chatcmpl-{int(time.time())}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": request.model or current_provider,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {
                                        "content": getattr(chunk, 'content', '') or getattr(getattr(chunk, 'choices', [{}])[0], 'delta', {}).get('content', '')
                                    },
                                    "finish_reason": getattr(chunk, 'finish_reason', None)
                                }
                            ]
                        }
                    
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                streaming_generator(),
                media_type="text/event-stream"
            )
        
        logger.info("Response received successfully")
        output_text = response.choices[0].message.content
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
        input_tokens = token_counter.count_tokens(str(messages), current_provider)
        output_tokens = token_counter.count_tokens(output_text, current_provider)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI API
        response_data = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model or current_provider,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": output_text
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": input_tokens,
                "completion_tokens": output_tokens,
                "total_tokens": input_tokens + output_tokens
            }
        }
        
        logger.info(f"Response formatted successfully. Tokens: {input_tokens}+{output_tokens}")
        return response_data
        
    except asyncio.TimeoutError:
        logger.error("Request timeout")
        raise HTTPException(status_code=504, detail="Request timeout")
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ—É—Ç–µ—Ä –∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é
from fastapi import APIRouter
v1_router = APIRouter(prefix="/v1")
app.include_router(v1_router)

# –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ endpoints –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
@app.get("/providers")
async def list_providers():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    return {"providers": list(providers.keys()), "current": current_provider}

@app.post("/switch-provider/{provider_name}")
async def switch_provider(provider_name: str):
    """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
    global current_provider
    if provider_name not in providers:
        raise HTTPException(status_code=400, detail=f"Provider {provider_name} not found")
    current_provider = provider_name
    return {"message": f"Switched to {provider_name}", "provider": current_provider}

# –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –æ—Ç–ª–æ–≤–∞ –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ Pydantic
from fastapi import Request
from fastapi.responses import JSONResponse
from pydantic import ValidationError

@app.exception_handler(ValidationError)
async def validation_exception_handler(request: Request, exc: ValidationError):
    body = await request.body()
    body_str = body.decode('utf-8', errors='ignore') if body else "empty"
    logger.error(f"‚ùå Pydantic validation error: {exc}")
    logger.error(f"‚ùå Request body length: {len(body_str)} characters")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    preview_length = min(500, len(body_str))
    logger.error(f"‚ùå Request body preview: {body_str[:preview_length]}")
    
    logger.error(f"‚ùå Error details: {exc.errors()}")
    logger.error(f"‚ùå Error type: {type(exc)}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
    import json
    try:
        parsed_body = json.loads(body_str) if body_str != "empty" else {}
        logger.error(f"‚ùå Parsed JSON keys: {list(parsed_body.keys()) if isinstance(parsed_body, dict) else 'Not a dict'}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É messages –µ—Å–ª–∏ –µ—Å—Ç—å
        if isinstance(parsed_body, dict) and 'messages' in parsed_body:
            messages = parsed_body['messages']
            logger.error(f"‚ùå Messages count: {len(messages) if isinstance(messages, list) else 'Not a list'}")
            if isinstance(messages, list) and messages:
                first_msg = messages[0]
                logger.error(f"‚ùå First message keys: {list(first_msg.keys()) if isinstance(first_msg, dict) else 'Not a dict'}")
                if isinstance(first_msg, dict) and 'content' in first_msg:
                    content = first_msg['content']
                    logger.error(f"‚ùå First message content type: {type(content)}")
                    if isinstance(content, str):
                        logger.error(f"‚ùå First message content preview: {content[:100]}...")
                
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå JSON decode error: {e}")
    
    return JSONResponse(
        status_code=422,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 422 –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º OpenAI API
        content={"detail": exc.errors(), "body_preview": body_str[:500]}
    )

# –î–æ–±–∞–≤–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"‚ùå Global exception: {exc}")
    logger.error(f"‚ùå Exception type: {type(exc)}")
    import traceback
    logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
    return JSONResponse(
        status_code=500,
        content={"detail": str(exc), "type": str(type(exc))}
    )