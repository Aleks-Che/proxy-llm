
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional, Union
import time
import logging
import asyncio
from providers.deepseek import DeepSeekProvider
from providers.moonshot import MoonshotProvider
from providers.local import LocalProvider
from utils.token_counter import TokenCounter
from config import config as Config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# Middleware –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
@app.middleware("http")
async def log_requests(request, call_next):
    client_ip = request.client.host if request.client else "unknown"
    user_agent = request.headers.get("user-agent", "unknown")
    logger.info(f"üîç Request from {client_ip}: {request.method} {request.url}")
    logger.info(f"üîç Headers: {dict(request.headers)}")
    
    # –¢–æ–ª—å–∫–æ –ª–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–ª–µ, –Ω–æ –Ω–µ —á–∏—Ç–∞–µ–º –µ–≥–æ
    if request.method == "POST":
        content_length = request.headers.get("content-length", "unknown")
        logger.info(f"üîç Content-Length: {content_length}")

    try:
        response = await call_next(request)
        logger.info(f"‚úÖ Response: {response.status_code}")
        logger.info(f"‚úÖ Response headers: {dict(response.headers)}")
        return response
    except Exception as e:
        logger.error(f"‚ùå Error in request processing: {e}")
        raise

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ü—Ä–æ–≤–∞–π–¥–µ—Ä—ã
providers = {}
provider_configs = Config.get_providers()

for provider_name, provider_config in provider_configs.items():
    if provider_config.get("enabled", False):
        api_key = provider_config.get("api_key", "")
        if provider_name == "deepseek" and api_key:
            providers["deepseek"] = DeepSeekProvider()
            logger.info("DeepSeek provider initialized")
        elif provider_name == "moonshot" and api_key:
            providers["moonshot"] = MoonshotProvider()
            logger.info("Moonshot provider initialized")
        elif provider_name == "local":
            providers["local"] = LocalProvider()
            logger.info("Local provider initialized")
        elif provider_name == "xai" and api_key:
            from providers.xai import XAIProvider
            providers["xai"] = XAIProvider()
            logger.info("xAI provider initialized")
        elif provider_name == "openrouter" and api_key:
            from providers.openrouter import OpenRouterProvider
            providers["openrouter"] = OpenRouterProvider()
            logger.info("OpenRouter provider initialized")
        elif provider_name == "anthropic" and api_key:
            from providers.anthropic import AnthropicProvider
            providers["anthropic"] = AnthropicProvider()
            logger.info("Anthropic provider initialized")

current_provider = Config.get_default_provider() if Config.get_default_provider() in providers else "local"
token_counter = TokenCounter()

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –ª–æ–≥–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤
request_logs = []
response_logs = []
MAX_LOGS = 100  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö—Ä–∞–Ω–∏–º—ã—Ö –ª–æ–≥–æ–≤

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
total_cost = 0.0

class ChatMessage(BaseModel):
    role: str
    content: Union[str, List[Dict[str, Any]], Dict[str, Any]]  # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ content
    
    class Config:
        extra = "allow"  # –†–∞–∑—Ä–µ—à–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è

class ChatCompletionRequest(BaseModel):
    model: Optional[str] = None
    messages: Optional[List[ChatMessage]] = None
    temperature: Optional[float] = 1.0
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False
    reasoning_effort: Optional[str] = None
    stream_options: Optional[Dict[str, Any]] = None
    top_p: Optional[float] = None
    presence_penalty: Optional[float] = None
    frequency_penalty: Optional[float] = None
    logit_bias: Optional[Dict[str, float]] = None
    user: Optional[str] = None
    suffix: Optional[str] = None
    echo: Optional[bool] = None
    best_of: Optional[int] = None
    logprobs: Optional[int] = None
    n: Optional[int] = None
    stop: Optional[List[str]] = None
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è –æ—Ç Cline
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[str] = None
    response_format: Optional[Dict[str, Any]] = None
    seed: Optional[int] = None
    max_completion_tokens: Optional[int] = None
    truncation_strategy: Optional[Dict[str, Any]] = None
    reasoning: Optional[Dict[str, Any]] = None
    parallel_tool_calls: Optional[bool] = None
    store: Optional[bool] = None
    metadata: Optional[Dict[str, Any]] = None
    
    # –†–∞–∑—Ä–µ—à–∞–µ–º –ª—é–±—ã–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –æ—Ç Cline
    class Config:
        extra = "allow"

@app.get("/")
async def root():
    return {"message": "LLM Proxy Server running", "provider": current_provider}

@app.get("/health")
async def health():
    return {"status": "healthy", "provider": current_provider}

@app.get("/test")
async def test():
    """–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
    try:
        provider = providers[current_provider]
        messages = [{"role": "user", "content": "test"}]
        response = await provider.chat_completion(messages, max_tokens=5)
        return {"status": "ok", "response": response.choices[0].message.content}
    except Exception as e:
        return {"status": "error", "error": str(e)}

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    logger.info("=== START PROCESSING ===")
    logger.info(f"Model requested: {request.model}")
    logger.info(f"Messages count: {len(request.messages) if request.messages else 0}")
    logger.info(f"Stream: {request.stream}")
    logger.info(f"Current provider: {current_provider}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ –ª–æ–≥
    user_message = ""
    if request.messages:
        for msg in request.messages:
            if msg.role == "user":
                if isinstance(msg.content, str):
                    user_message = msg.content
                elif isinstance(msg.content, list):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                    for item in msg.content:
                        if isinstance(item, dict) and item.get('type') == 'text':
                            user_message = item.get('text', '')
                            break
                break
    
    request_log = {
        "timestamp": time.time(),
        "provider": current_provider,
        "user_message": user_message[:500] + "..." if len(user_message) > 500 else user_message,
        "messages_count": len(request.messages) if request.messages else 0,
        "stream": request.stream
    }
    
    global request_logs
    request_logs.append(request_log)
    if len(request_logs) > MAX_LOGS:
        request_logs = request_logs[-MAX_LOGS:]
    
    try:
        provider = providers[current_provider]
        logger.info(f"Using provider: {current_provider}")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π - –±–æ–ª–µ–µ –≥–∏–±–∫–∞—è
        messages = []
        if request.messages:
            logger.info(f"Raw messages: {request.messages}")
            for i, msg in enumerate(request.messages):
                logger.info(f"Message {i}: role={msg.role}, content_type={type(msg.content)}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º content –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
                content = msg.content
                
                # –ï—Å–ª–∏ content - —Å—Ç—Ä–æ–∫–∞
                if isinstance(content, str):
                    processed_content = content
                # –ï—Å–ª–∏ content - —Å–ø–∏—Å–æ–∫ (–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
                elif isinstance(content, list):
                    text_parts = []
                    for item in content:
                        if isinstance(item, dict):
                            if item.get("type") == "text":
                                text_parts.append(item.get("text", ""))
                            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º image_url –∏ –¥—Ä—É–≥–∏–µ —Ç–∏–ø—ã –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
                        elif isinstance(item, str):
                            text_parts.append(item)
                    processed_content = " ".join(text_parts)
                # –ï—Å–ª–∏ content - dict
                elif isinstance(content, dict):
                    processed_content = str(content)  # –ò–ª–∏ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ dict
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤
                else:
                    processed_content = str(content)
                
                messages.append({
                    "role": msg.role,
                    "content": processed_content
                })
                
                logger.info(f"Processed message {i}: {processed_content[:100]}...")
        
        logger.info(f"Processed messages count: {len(messages)}")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        kwargs = {"max_tokens": 1000}
        if request.max_tokens:
            kwargs["max_tokens"] = request.max_tokens
        if request.temperature is not None:
            kwargs["temperature"] = request.temperature
        if request.stream:
            kwargs["stream"] = True
            
        logger.info(f"Calling provider with {len(messages)} messages and kwargs: {kwargs}")
        
        # –í—ã–∑–æ–≤ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        response = await asyncio.wait_for(
            provider.chat_completion(messages, **kwargs),
            timeout=120.0  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        )
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ streaming response
        if request.stream:
            logger.info("Streaming response requested")
            logger.info(f"Stream options: {request.stream_options}")
            logger.info(f"Reasoning effort: {request.reasoning_effort}")
            
            # –î–ª—è streaming –æ—Ç–≤–µ—Ç–æ–≤ —Å–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
            from fastapi.responses import StreamingResponse
            import json
            
            async def streaming_generator():
                # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è usage —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                input_tokens = token_counter.count_tokens(str(messages), current_provider)
                completion_tokens = 0
                accumulated_content = ""
                
                async for chunk in response:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ —á–∞–Ω–∫–∞ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
                    content = ""
                    if hasattr(chunk, 'content'):
                        content = getattr(chunk, 'content', '')
                    elif hasattr(chunk, 'choices') and getattr(chunk, 'choices'):
                        choices = getattr(chunk, 'choices')
                        if choices and hasattr(choices[0], 'delta'):
                            delta = getattr(choices[0], 'delta')
                            if hasattr(delta, 'content'):
                                content = getattr(delta, 'content', '')
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º accumulated_content –∏ completion_tokens
                    if content:
                        accumulated_content += content
                        completion_tokens = token_counter.count_tokens(accumulated_content, current_provider)
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º chunk –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç
                    if hasattr(chunk, 'model_dump'):
                        chunk_dict = chunk.model_dump()
                    elif hasattr(chunk, 'to_dict'):
                        chunk_dict = chunk.to_dict()
                    elif hasattr(chunk, 'dict'):
                        chunk_dict = chunk.dict()
                    else:
                        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ñ–æ—Ä–º–∞—Ç–æ–º Cline
                        chunk_dict = {
                            "id": f"chatcmpl-{int(time.time())}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": request.model or current_provider,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {
                                        "content": content,
                                        "reasoning_content": None  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Cline
                                    },
                                    "finish_reason": getattr(chunk, 'finish_reason', None)
                                }
                            ]
                        }
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º completion_tokens –≤–æ –≤—Å–µ—Ö —á–∞–Ω–∫–∞—Ö
                    # –î–ª—è OpenRouter –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º usage –≤ –∫–∞–∂–¥—ã–π chunk –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π API
                    if request.stream_options and request.stream_options.get("include_usage", False) and current_provider != "openrouter":
                        chunk_dict["usage"] = {
                            "prompt_tokens": input_tokens,
                            "completion_tokens": completion_tokens,
                            "total_tokens": input_tokens + completion_tokens,
                            "prompt_tokens_details": {
                                "cached_tokens": 0  # –ü–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
                            },
                            "prompt_cache_miss_tokens": input_tokens
                        }
                    
                    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ JSON –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω
                    try:
                        json_str = json.dumps(chunk_dict, ensure_ascii=False)
                        yield f"data: {json_str}\n\n"
                    except Exception as e:
                        logger.error(f"JSON serialization error: {e}")
                        yield f"data: {{\"error\": \"JSON serialization failed\"}}\n\n"
                
                # –§–∏–Ω–∞–ª—å–Ω—ã–π chunk —Å –ø–æ–ª–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π usage
                # –î–ª—è OpenRouter –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π usage chunk –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π API
                if request.stream_options and request.stream_options.get("include_usage", False) and current_provider != "openrouter":
                    final_chunk = {
                        "id": f"chatcmpl-{int(time.time())}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": request.model or current_provider,
                        "choices": [
                            {
                                "index": 0,
                                "delta": {},
                                "finish_reason": "stop"
                            }
                        ],
                        "usage": {
                            "prompt_tokens": input_tokens,
                            "completion_tokens": completion_tokens,
                            "total_tokens": input_tokens + completion_tokens,
                            "prompt_tokens_details": {
                                "cached_tokens": 0
                            },
                            "prompt_cache_miss_tokens": input_tokens
                        }
                    }
                    try:
                        json_str = json.dumps(final_chunk, ensure_ascii=False)
                        yield f"data: {json_str}\n\n"
                    except Exception as e:
                        logger.error(f"Final chunk JSON error: {e}")
                
                # –†–∞—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –¥–ª—è streaming –∑–∞–ø—Ä–æ—Å–∞
                request_cost = token_counter.estimate_cost(input_tokens, completion_tokens, current_provider)
                global total_cost
                total_cost += request_cost

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ –ª–æ–≥ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è streaming
                response_log = {
                    "timestamp": time.time(),
                    "provider": current_provider,
                    "response": accumulated_content[:1000] + "..." if len(accumulated_content) > 1000 else accumulated_content,
                    "input_tokens": input_tokens,
                    "output_tokens": completion_tokens,
                    "cost": request_cost
                }

                global response_logs
                response_logs.append(response_log)
                if len(response_logs) > MAX_LOGS:
                    response_logs = response_logs[-MAX_LOGS:]
                    logger.info(f"Saved streaming response to log: {accumulated_content[:100]}...")
                
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                streaming_generator(),
                media_type="text/event-stream"
            )
        
        logger.info("Response received successfully")
        output_text = response.choices[0].message.content
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
        input_tokens = token_counter.count_tokens(str(messages), current_provider)
        output_tokens = token_counter.count_tokens(output_text, current_provider)

        # –†–∞—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –¥–ª—è –ø–ª–∞—Ç–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        request_cost = token_counter.estimate_cost(input_tokens, output_tokens, current_provider)
        global total_cost
        total_cost += request_cost

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI API
        response_data = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model or current_provider,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": output_text
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": input_tokens,
                "completion_tokens": output_tokens,
                "total_tokens": input_tokens + output_tokens
            }
        }
        
        logger.info(f"Response formatted successfully. Tokens: {input_tokens}+{output_tokens}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç –≤ –ª–æ–≥
        response_log = {
            "timestamp": time.time(),
            "provider": current_provider,
            "response": output_text[:1000] + "..." if len(output_text) > 1000 else output_text,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": request_cost
        }

        global response_logs
        response_logs.append(response_log)
        if len(response_logs) > MAX_LOGS:
            response_logs = response_logs[-MAX_LOGS:]
        
        return response_data
        
    except asyncio.TimeoutError:
        logger.error("Request timeout")
        raise HTTPException(status_code=504, detail="Request timeout")
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ—É—Ç–µ—Ä –∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é
from fastapi import APIRouter
v1_router = APIRouter(prefix="/v1")
app.include_router(v1_router)

# –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ endpoints –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
@app.get("/providers")
async def list_providers():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    return {"providers": list(providers.keys()), "current": current_provider}

@app.post("/switch-provider/{provider_name}")
async def switch_provider(provider_name: str):
    """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
    global current_provider
    if provider_name not in providers:
        raise HTTPException(status_code=400, detail=f"Provider {provider_name} not found")
    current_provider = provider_name
    return {"message": f"Switched to {provider_name}", "provider": current_provider}

@app.post("/debug/cline")
async def debug_cline_request(request: Request):
    """–≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç Cline"""
    body = await request.body()
    body_str = body.decode('utf-8', errors='ignore')
    
    try:
        import json
        parsed = json.loads(body_str)
    except:
        parsed = {"error": "Invalid JSON"}
    
    return {
        "headers": dict(request.headers),
        "body": parsed,
        "raw_body": body_str,
        "method": request.method,
        "url": str(request.url)
    }

# Endpoints –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ª–æ–≥–æ–≤
@app.get("/logs/requests")
async def get_request_logs():
    """–ü–æ–ª—É—á–∏—Ç—å –ª–æ–≥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    return {"request_logs": request_logs}

@app.get("/logs/responses")
async def get_response_logs():
    """–ü–æ–ª—É—á–∏—Ç—å –ª–æ–≥–∏ –æ—Ç–≤–µ—Ç–æ–≤"""
    return {"response_logs": response_logs}

@app.get("/logs/all")
async def get_all_logs():
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –ª–æ–≥–∏ (–∑–∞–ø—Ä–æ—Å—ã + –æ—Ç–≤–µ—Ç—ã)"""
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    all_logs = []
    for log in request_logs:
        all_logs.append({"type": "request", **log})
    for log in response_logs:
        all_logs.append({"type": "response", **log})
    
    all_logs.sort(key=lambda x: x["timestamp"])  # reverse=False –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    return {"logs": all_logs[-50:]}  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 50 (–Ω–æ–≤—ã–µ)

# Endpoint –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å GUI)
@app.get("/stats")
async def get_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ—Ä–≤–µ—Ä–∞"""
    global total_cost
    return {
        "total_requests": len(request_logs),
        "total_tokens": sum(log.get("input_tokens", 0) + log.get("output_tokens", 0) for log in response_logs),
        "total_cost": total_cost,
        "requests": []  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º GUI
    }

# –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –æ—Ç–ª–æ–≤–∞ –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ Pydantic
from fastapi import Request
from fastapi.responses import JSONResponse
from pydantic import ValidationError

@app.exception_handler(ValidationError)
async def validation_exception_handler(request: Request, exc: ValidationError):
    body = await request.body()
    body_str = body.decode('utf-8', errors='ignore') if body else "empty"
    
    logger.error("=== PYDANTIC VALIDATION ERROR ===")
    logger.error(f"Request body length: {len(body_str)}")
    logger.error(f"Request body preview: {body_str[:1000]}")
    logger.error(f"Validation errors: {exc.errors()}")
    
    # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    try:
        import json
        parsed = json.loads(body_str)
        logger.error(f"Parsed JSON keys: {list(parsed.keys())}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º messages
        if 'messages' in parsed:
            messages = parsed['messages']
            logger.error(f"Messages count: {len(messages)}")
            if messages:
                first_msg = messages[0]
                logger.error(f"First message keys: {list(first_msg.keys())}")
                if 'content' in first_msg:
                    content = first_msg['content']
                    logger.error(f"Content type: {type(content)}")
                    if isinstance(content, str):
                        logger.error(f"Content preview: {content[:200]}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø–æ–ª—è
        known_fields = {
            'model', 'messages', 'temperature', 'max_tokens', 'stream',
            'reasoning_effort', 'stream_options', 'top_p', 'presence_penalty',
            'frequency_penalty', 'logit_bias', 'user', 'suffix', 'echo',
            'best_of', 'logprobs', 'n', 'stop', 'tools', 'tool_choice',
            'response_format', 'seed', 'max_completion_tokens', 'truncation_strategy',
            'reasoning', 'parallel_tool_calls', 'store', 'metadata'
        }
        unknown_fields = set(parsed.keys()) - known_fields
        if unknown_fields:
            logger.error(f"Unknown fields from Cline: {unknown_fields}")
            
    except Exception as e:
        logger.error(f"Error parsing request body: {e}")
    
    # –í–º–µ—Å—Ç–æ 422 –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 200 —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
    return JSONResponse(
        status_code=200,
        content={
            "error": "Validation failed - check logs",
            "cline_request": body_str[:500],
            "validation_errors": exc.errors()
        }
    )

# –î–æ–±–∞–≤–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"‚ùå Global exception: {exc}")
    logger.error(f"‚ùå Exception type: {type(exc)}")
    import traceback
    logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
    return JSONResponse(
        status_code=500,
        content={"detail": str(exc), "type": str(type(exc))}
    )